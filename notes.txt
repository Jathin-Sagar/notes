DevOps Notes
<<lsblk
lsblk to display block devices
lsblk -a To display empty block devices.  
lsblk -b print size in bits
lsblk -z print zone model
lsblk -d skip slave entries
lsblk -i to use ascii characters for tree formation
lsblk -m  To print information about device owner, group, and mode of block devices.  
lsblk -o SIZE,NAME,MOUNTPOINT To print selected columns of block-devices. 
lsblk -dn hide column headings
lsblk -help display help section
lsblk
<<tar 
(tape archive)
the command iThe tar command is often used for creating compressed archive files that can be easily transferred or stored. It can compress multiple files into a single archive file, and the resulting archive can be compressed using different compression algorithms like gzip, bzip2, and xz to reduce its size.s used for creating archive and extracting the archive files it was originally used to create backups to tape drives.
tar cvf create,verbose so it gets update, filename(flag) file.tar hello.txt it shown in the red colour, it is a uncompresed tar archive.
tar xvf file.tar extracts files through archives
tar cvzf file.tar.gz hello.txt it makes a tar file kown as file.tar.gz it is the archive of hello.txt file
tar xvzf file.tar.gz it is used to extract the files through file.tar.gz archives file
tar cvfj file.tar.tbz hello.txt making compressed tar files with the -j option
tar -xvf file.tar -c / tar -xvf file.tar  path of directory used to untar( command which enable the users to extract files that are compressed with tar ,tar.gz)
tar -jxvf file.tar.tbz "hello.txt" untar multiple .tar.tbz, .tar.gz, .tar extract more than one file
tar -czf - file.tar./gz/tbz | wc -c it is used to see the size of archive files in kilobytes
tar rvf file.tar hello.txt updating an existing tar file
tar tf file.tar list the whole archive files
tar -tvf file.tar view the archive
tar tvf file.tar hello.txt  view the archived files with their information
tar tvf file.tar | grep "file.txt" help us to only list the mentioned image or text in grep through archived files.
wildcards are referred to as a wildcard character or wild character in Linux. It is a symbol that is used for representing or replacing multiple characters.
Typically, wildcards are either a question mark (?) which illustrates an individual character or an asterisk (*) which illustrates multiple characters.
tar tvf file.tar --wildcards '*.png' finding a .png image
 tar --delete -f file.tar hello.txt to remove files and a tar archive.
tar
<<cronjobs  

A scheduling tool is a software application or program used to automate and manage tasks that need to be performed at specific times or intervals. Scheduling tools are commonly used to streamline and optimize workflows, reduce errors, and save time and effort for users.

There are many types of scheduling tools available, ranging from simple task scheduling tools like the Unix cron utility to more complex project management software and workflow automation platforms.

Scheduling tools typically allow users to set up recurring tasks, assign resources, and set deadlines or timelines for completion. They may also include features for tracking progress, sending notifications or reminders, and generating reports or analytics.

Some common types of scheduling tools include:

Calendar and appointment scheduling tools: These tools allow users to schedule appointments, meetings, and other events. They may include features for sending reminders, tracking attendance, and rescheduling appointments if needed.

Task scheduling tools: These tools allow users to schedule and automate routine tasks, such as data backups, software updates, or system maintenance tasks.

Project management tools: These tools are designed to manage project timelines, deadlines, and milestones. They may include features for scheduling tasks, assigning resources, and tracking progress towards project goals.

Workforce management tools: These tools are used to manage staff schedules, assign tasks, and track employee time and attendance. They may also include features for managing payroll, benefits, and other HR functions.

Overall, scheduling tools are essential for managing complex workflows and ensuring that tasks are completed efficiently and on time. They are used in a wide range of industries and settings, including business, education, healthcare, and government.
Crontab Operators
Cron syntax also utilizes operators for performance. Operators are large inland that operates effectively on the Cron attribute values. The operators are discussed below-

Asterisk operator (*)
The asterisk operator denotes any significance or already. Suppose you see an asterisk (*) in the Hour domain, it implies the job will be done every hour. It represents all values. Utilization of this operator is to gather operating for the entire month or week.

Comma Operator (,)
You can stipulate a range of items for regurgitation using the comma operator. It also defines distinct unique values.

For Instance, if you enter 2,5,8 in the Hour domain, the assignment will execute at 2 a.m., 5 a.m., and 8 a.m.

Hyphen Operator (-)
You can stipulate a set of outcomes using the hyphen operator. If you enter 2-5 in the Weekday domain, the assignment will execute every weekday (From Tuesday to Friday). It also represents a set of parameters.

Forward slash Operator (/)
The slash operator allows you to specify values that will be repeated over a specific interval between them. This operator can also be used to separate a number into various stages.

For example, if you have */4 in the Hour field, it means the action will be performed every four hours.

It is also equivalent as clarifying 0,4,8,12,16,20. You can utilize a range of data rather than an asterisk even before the slash operator.

For example, 1-30/10 indicates the similar as 1,11,21.

<<unzip
through cmd and also in the system (graphical interface)we can unzip the files,we can extracting compressed files from a ZIP archive. ZIP files are a popular format for compressing and packaging files together, making them easier to transfer and store.
The primary reason for using ZIP files is to save storage space and reduce transfer. 
unzip archive.zip this command is used to unzip the files
unzip archive.zip -d myfiles this command is used to unzip file and save in a directory
unzip archive.zip file.txt unzipig the file and saving as text files
unzip

<<sed
The sed command (short for "stream editor") is a powerful utility in Linux and other Unix-based operating systems that can be used to modify text files.

The sed command is designed to read input files (either from standard input or from one or more files) and apply a set of commands to the text. The commands can be used to perform a wide range of text transformations, such as:

Replacing text: You can use the s command to search for a pattern in the text and replace it with a new string. For example, the command sed 's/hello/goodbye/' will replace all occurrences of the word "hello" with "goodbye" in the input text.

Deleting lines: You can use the d command to delete lines that match a particular pattern. For example, the command sed '/pattern/d' will delete all lines that contain the pattern.

Inserting text: You can use the i command to insert new text before a specified line. For example, the command sed '2i\New line' will insert the text "New line" before the second line of the input text.

Appending text: You can use the a command to append new text after a specified line. For example, the command sed '2a\New line' will append the text "New line" after the second line of the input text.

Transforming text: You can use regular expressions and other sed commands to transform text in a wide range of ways.

The sed command can be used in combination with other Linux utilities to perform complex text processing tasks. It is a powerful tool for manipulating text files and automating text processing tasks on the command line
echo class7 | sed 's/class/jtp/'  here class is changed to jtp
echo class7 | sed 's/7/10/'    in this 7 is changed to 10
 cat msg.txt | sed 's/learn/study/'    in the text file we changed lern word to study
echo class7 class9 | sed 's/class/jtp/g' here /' it only edits one file /g' it changes all the files to jtp   
cat msg.txt | sed 's/learn/study/g'   it only edits one text /g' it changes all the texts to study
cat msg.txt | sed '/jtp/d'    deleting line of jtp    
sed -e 's/red/blue/; s/yellow/black/' exm.txt  modifing multiple sed command
sed -f SedCommands exm.txt   we have used commands applied to the earlier example. So, the output is the same as the previous example
sed '3s/Red/Blue/' exm.txt  at which line we need to change we use this red is changed to blue
echo "Another Demo" | sed 'i\First Demo'  it will append the text
sed '3c\This is a modified line.' exm.txt  modifing the lines 
sed 'y/abc/def/' exm.txt  transfering the characters
sed '=' exm.txt  printing the line number
sed -n '/mango/=' exm.txt  it will display the line number with the word mango
sed
<<awk
The awk command is used for text processing in Linux. Although, the sed command is also used for text processing, but it has some limitations, so the awk command becomes a handy option for text processing. It provides powerful control to the data.

The Awk is a powerful scripting language used for text scripting. It searches and replaces the texts and sorts, validates, and indexes the database.
awk '{ print "Welcome to awk command"}' The above command will print the inputted string every time we execute the command. Press CTRL+D key to terminate the program
cat > student.txt  
Sam CS  
Daniel IT  
John IT  
Arya IT  
Mike ECE  
Helena ECE  
awk '/cs/ {print}' student.txt list the students with specific patterns like cs ece first we need to create file with cat
awk '{print}' student.txt to see all the files
awk '{print $1,$5}' student.txt this command will print the column 1 and 5
NR: It is used to show the current count of the lines. The awk command performs action once for each line. These lines are said as records.

NF: It is used to count the number of fields within the current database.

FS: It is used to create a field separator character to divide fields into the input lines.

OFS: It is used to store the output field separator. It separates the output fields.

ORS: It is used to store the output record separator. It separates the output records. It prints the content of the ORS command automatically.
awk '{print NR $2}' student.txt display line number and column of the given number
awk '{print $NF}' student.txt print the last fileld of the file
awk 'BEGIN { ORS ="-"} {print $0}' student.txt seperate the output by _underscore
awk 'BEGIN { for(i=1;i<=8;i++) print "square of", i, "is",i*i; }' print the square of 1 to 8.
Sam,75,100  
Daniel,80,100  
John,74,100  
Arya,85,100  
Mike,70,100   
Helena,74,100 
awk -F"," '{x+=$3}END{print x}' student.txt  to find out the total marks of the students
awk -F, '{a[$1]+=$2;}END{for(i in a)print i", "a[i];}' student.txt print the individual's name with his marks
awk 'BEGIN{x=exp(8); print x}'   print the value of exp 8

awk
<<ls
ls > bharath in this all the files contains in the directory is shown in bharath with cat jathin we can see it
1>&and percentile directory (standared output stdout)/2 &>and percentile directory (standared error stderr) sai now the file is getting created we can see it by using less or more command but , we can use gedit it opens the file in the folder  , 1 > sai for this we get the command not found
ls
1 > filename 2 >& 1 in this both the standard output and standard error is send to the file name
<<uname
The command ‘uname‘ displays the information about the system
uname -a  It prints all the system information in the following order: Kernel name, network node hostname, kernel release date, kernel version, machine hardware name, hardware platform, operating system
uname -s print kernal name
uname -n print hostname
uname -r print kernal release date
uname -v version of the current kernel
uname -m print machine hardware name
uname -p print machine processor
uname -i print the platform of hardware
uname -o print the name of the operating system
uname

<<$
echo is used infront of all the variables
$$ pid of the current shell
$! pid if last background command
$? existing status of the last command executed
$0 filenam eof the current script
$n/1/2 these variable corresponds to the argument with which a script was invoked.her n is a positive decimal number corresponding to the position of an argument ex 1st is 1 2nd s 2
$# number of arguments supplied to a script
$* all the arguments are double quotes,if the script recives 2 arguments then $* is equalent to $1 $2
$@all the arguments are individually double quoted if a script recives two arguments $@ is equivalent to$1 $2
$SHELL is a special shell variable that contains the pathname of the user's preferred shell.
cat /etc/shells  The list of all the shells which are currently installed in our Linux system is stored in the ‘shells’ file which is present in /etc folder of the system. It has read-only access by default and is modified automatically whenever we install a new shell in our system. As we can see, the cat command displays the various installed shells along with their installation paths..
In Unix-like operating systems, echo {A} and echo "{A}" are different in terms of how the shell performs brace expansion and variable expansion.
<< echo {A} and echo "{A}"
echo {A} is an example of brace expansion, which is a feature of many Unix shells that allows you to generate a list of strings by specifying a pattern inside curly braces. In this case, the pattern is a single character "A", so brace expansion generates a list that contains just the string "A". When you run the echo command with this argument, it simply prints the string "A" to the terminal.

echo "{A}" is an example of a string literal that contains the character sequence "{A}". When you run the echo command with this argument, it simply prints the string "{A}" to the terminal, without any further expansion or processing.

In summary, echo {A} generates a list of strings using brace expansion and prints the resulting string to the terminal, while echo "{A}" simply prints a string literal to the terminal without any expansion.
echo {A} and echo "{A}"
{A} it has some expansion ,"{A}" it don't have any expansion

<< #!/bin/bash
#!/bin/ env bash
The main difference between the two is that #!/usr/bin/bash specifies the absolute path to the Bash interpreter, while #!/usr/bin/env bash uses the env command to locate the Bash interpreter in the user's PATH environment variable.Using #!/usr/bin/bash assumes that Bash is installed in the /usr/bin/ directory, which may not always be the case on different systems. On some systems, Bash may be installed in a different directory, which could cause the script to fail.

Using #!/usr/bin/env bash is a more portable option as it searches for the Bash interpreter in the PATH environment variable, which is a list of directories that the shell searches for executables. This makes the script more adaptable to different systems and makes it easier to maintain.
#!/bin/env bash
#!/bin/bash

<<verify user root or not
The command $(id -u)" -eq 0 is used to check if the current user has root privileges (i.e., is the superuser or administrator).

In Unix-based systems, every user is assigned a unique numeric user ID (UID) that is used to identify the user. The root user (superuser or administrator) has a UID of 0, while regular users have UIDs that are greater than 0.

The id -u command is used to retrieve the UID of the current user. The $() syntax is used to capture the output of the command and use it as a variable. The expression " -eq 0" is a comparison operator that checks if the UID is equal to 0.

So, when the command $(id -u)" -eq 0 is executed, it returns a value of 0 if the current user has root privileges, and a value of 1 (or some other non-zero value) if the current user does not have root privileges.

This command is commonly used in shell scripts to perform actions that require root privileges, such as installing software, modifying system settings, or accessing restricted files and directories. By checking if the current user has root privileges before performing such actions, the script can avoid potential errors or security risks.
verify user root or not

#difference between [] and [[]]
[ and [[ can be used for conditional expressions, [[ is a more powerful and flexible option that is specific to Bash. [ is more portable and can be used in other shells.
#set -n, set -x, set -e, and set +x are all options that can be used in Bash shell scripts to modify the behavior of the shell.

set -n (or set -o noexec) prevents the execution of commands in the script. When this option is set, the shell reads and checks the syntax of the script, but does not execute any of the commands. This can be useful for checking the syntax of a shell script without actually running it.

set -x (or set -o xtrace) enables tracing of commands in the script. When this option is set, the shell prints each command to the terminal as it is executed, preceded by the value of the PS4 variable (which defaults to +).

set -e (or set -o errexit) causes the script to exit immediately if any command exits with a non-zero status (i.e., if an error occurs). This can be useful for catching errors early in the script and preventing unintended consequences.

set +x disables tracing of commands in the script. This option is used to turn off the set -x option when it is no longer needed.

It is worth noting that these options are not exclusive and can be used together in a script. For example, it is common to use set -e and set -x together to catch errors and trace the execution of a script at the same time. The options can be set and unset as needed throughout the script using the set command.

<</etc/profile 

/etc/profile is a system-wide initialization script for Bash that is executed for all users when they log in to the system. It is located in the /etc directory, which is typically only accessible by the system administrator.

/etc/profile is used to set environment variables and aliases that are available to all users on the system. It may also contain system-wide settings and configurations for various applications and services.

In general, /etc/profile is used to set global environment variables, such as the PATH variable, which specifies the directories that the system will search when looking for executable programs. It may also set variables that are specific to certain applications or services that are installed on the system.

Since /etc/profile is executed for all users when they log in, it is important to be careful when making changes to this file, as they may affect all users on the system. It is a good practice to back up the file before making any changes, and to test any changes in a non-production environment before applying them to a live system.
  --it gives us inbuilt information about the root privileges in a script formate.
/etc/profile 

<<uniq
The uniq command in Linux is a command-line utility that reports or filters out the repeated lines in a file. 
In simple words, uniq is the tool that helps to detect the adjacent duplicate lines and also deletes the duplicate lines. uniq filters out the adjacent matching lines from the input file(that is required as an argument) and writes the filtered data to the output file. 
uniq murali.txt example
 

-c – -count : It tells how many times a line was repeated by displaying a number as a prefix with the line.
-d – -repeated : It only prints the repeated lines and not the lines which aren’t repeated.
-D – -all-repeated[=METHOD] : It prints all duplicate lines and METHOD can be any of the following: 
none : Do not delimit duplicate lines at all. This is the default.
prepend : Insert a blank line before each set of duplicated lines.
separate : Insert a blank line between each set of duplicated lines.
-f N – -skip-fields(N) : It allows you to skip N fields(a field is a group of characters, delimited by whitespace) of a line before determining the uniqueness of a line.
-i – -ignore case : By default, comparisons done are case sensitive but with this option case insensitive comparisons can be made.
-s N – -skip-chars(N) : It doesn’t compare the first N characters of each line while determining uniqueness. This is like the -f option, but it skips individual characters rather than fields.
-u – -unique : It allows you to print only unique lines.
-z – -zero-terminated : It will make a line end with 0 bytes (NULL), instead of a newline.
-w N – -check-chars(N) : It only compares N characters in a line.
– – help : It displays a help message and exit.
– – version : It displays version information and exit.

uniq
 
<<alias
alias command instructs the shell to replace one string with another string while executing the commands. 

When we often have to use a single big command multiple times, in those cases, we create something called as alias for that command. Alias is like a shortcut command which will have same functionality as if we are writing the whole command. 
alias 1="cd Documents"
unalias 1
alias -p it will show all the aliases

alias
<<sort
sort weeks.txt  to sort the files in alphabetical order.
sort -k1 states.txt If a file has more than one column, column number is used to sort a specific column.
sort -n -k2 marks.txt Numeric sorting is different from alphabetical sorting. For numeric sorting option 'n' is used along with the column number if required.

sort

<<uniq
sort dupil.txt | uniq remove repeated lines
sort dupil.txt | uniq -c counts the number of times a word is repeating.
sort dupil.txt | uniq -d only the displayed lines 
sort dupil.txt | uniq -u only display the uniq lines
sort dupil.txt | uniq -s 2 ignore the first 2 characters 
uniq -f 2 dupil.txt The above command will not compare the first two fields from the file dupli.txt.

uniq

<< port number
A port number is a 16-bit unsigned integer that identifies a specific process to which a network message should be delivered within a computer network. It is used to distinguish between different network services that may be running on the same network device, such as a computer or a router.

port number         servicename        transport protocol         Description
7                   Echo                TCP,UDP                   echo services
21                  FTP                 TCP,SCTP                  file transfer protocol data transfer
22                  SSH-SCP             TCP,UDP,SCTP              Secure Shell, secure logins, file transfers                                                                       (scp, sftp), and port forwarding
23                  TELNET              TCP,UDP,SCTP              Telnet protocol—unencrypted text communications
25                  SMTP                TCP                       Simple Mail Transfer Protocol, used for email                                                                     routing between mail servers
53                  DNS                 TCP,UDP                   Domain Name System name resolver
80                  HTTP                TCP,UDP,SCTP              Hypertext Transfer Protocol (HTTP) uses TCP in                                                                     versions 1.x and 2. HTTP/3 uses QUIC, a                                                                           transport protocol on top of UDP
443                 HTTPS over SSL      TCP,UDP,SCTP              Hypertext Transfer Protocol Secure (HTTPS) uses                                                                   TCP in versions 1.x and 2. HTTP/3 uses QUIC, a                                                                     transport protocol on top of UDP.
465                 SMTP over TLS/SSL,  TCP                       Authenticated SMTP over TLS/SSL (SMTPS), URL                         SSM                                           Rendezvous Directory for SSM (Cisco protocol)
587                 SMTP                TCP                       Email message submission
989                 FTP over SSL        TCP,UDP                   FTPS Protocol (data), FTP over TLS/SSL
990                 FTP over SSL        TCP,UDP                   FTPS Protocol (control), FTP over TLS/SSL
The FTPS data channel is used for transmitting file data, while the FTPS control channel is used for transmitting control information about the file transfer process. Both channels are encrypted using SSL/TLS to ensure the confidentiality and integrity of the data being transmitted.
1194                OpenVPN	            TCP,UDP                  	OpenVPN
3306                MySQL	              TCP                     	 MySQL database system
3389                RDP                RemoteDesktopProtocol(RDP) . RDP enables users to remotely connect to their                                                                   desktop computers from another device.
5432	               PostgreSQL          TCP	                      PostgreSQL database system



port
<<tcp
TCP (Transmission Control Protocol) is used for providing reliable and ordered delivery of data between applications running on hosts communicating over an IP network. Some of the main uses of TCP are:
Web browsing: When you visit a website using a web browser such as Chrome or Firefox, the browser establishes a TCP connection with the web server to request and receive the web page content. TCP ensures that the content is delivered reliably and in the correct order.
Email: When you send or receive email, your email client (such as Outlook or Gmail) uses TCP to establish a connection with the mail server and exchange email messages reliably and in the correct order.
File transfers: When you download or upload files over the internet using protocols such as FTP or SFTP, TCP is used to ensure that the files are transferred reliably and in the correct order.
Online gaming: Many online games use TCP to send and receive data between game clients and servers, to ensure that the game data is delivered reliably and in the correct order.
Overall, TCP is widely used in applications that require reliable and ordered data delivery, where the integrity and order of the data are critical for the proper functioning of the application.
tcp
<<udp
UDP (User Datagram Protocol) is a connectionless transport layer protocol in the Internet Protocol (IP) suite. Unlike TCP, UDP does not provide reliable delivery of data or guarantee that the data will be received in the correct order. UDP is used in applications where speed and efficiency are more important than reliability and ordered delivery.
Some common uses of UDP are:
Real-time multimedia applications: Applications such as voice over IP (VoIP), video conferencing, and online gaming often use UDP to transmit real-time data because of its low latency and fast transmission speed. In these applications, it is more important to have real-time delivery of data than to ensure that every packet is received and in order.
DNS (Domain Name System): UDP is used by the DNS protocol to quickly resolve domain names to IP addresses. When you type a domain name into your web browser, your computer sends a DNS query to a DNS server over UDP to obtain the IP address of the server that hosts the website.
DHCP (Dynamic Host Configuration Protocol): UDP is used by DHCP to assign IP addresses to devices on a network. When a new device joins a network, it sends a DHCP request over UDP to a DHCP server, which assigns it an IP address.
SNMP (Simple Network Management Protocol): SNMP is used to monitor and manage network devices such as routers, switches, and servers. UDP is used by SNMP to send management information between devices.
Overall, UDP is used in applications where speed and efficiency are more important than reliability and ordered delivery, and where occasional loss or duplication of packets is acceptable.
udp

<<sctp
SCTP (Stream Control Transmission Protocol) is a transport layer protocol that is designed to provide reliable, ordered, and flow-controlled delivery of messages between two endpoints. SCTP was developed as an alternative to TCP and UDP and is used in applications that require higher reliability and better congestion control than TCP, while also providing multi-homing support and improved security compared to UDP.
Some common uses of SCTP are:
Telecommunications: SCTP is used in telecommunications networks to transport signaling messages between network elements, such as switches and gateways. The reliable, ordered, and flow-controlled delivery of messages provided by SCTP ensures that signaling messages are delivered correctly and in the right order.
Real-time multimedia applications: SCTP is also used in real-time multimedia applications, such as voice and video conferencing, where the reliability and ordered delivery of messages is critical for maintaining the quality of the media stream.
Electronic payment systems: SCTP is used in electronic payment systems, such as credit card payment gateways, to ensure that payment transactions are processed securely and reliably.
Industrial control systems: SCTP is used in industrial control systems to transport control messages between devices in industrial networks. The reliable and ordered delivery of messages provided by SCTP is important for ensuring the safety and reliability of industrial processes.
Overall, SCTP is used in applications that require high reliability and improved congestion control, while also providing multi-homing support and improved security.

sctp
<<ifup
The ifup command basically brings the network interface up, allowing it to transmit and receive data. Technically ifup command is used to configure network interfaces based on interface definitions in the file /etc/network/interfaces.
sudo ifup -av , sudo is used for permissions, -a to work on all devices and v for verbose the output.
-a(–all): If used this option it affects all the interfaces marked as auto. Brings up interfaces in order they are defined /etc/network/interfaces. Combined with –allow, acts on all interfaces of a specified class instead.
–force: Force configuration or deconfiguration of the interface.
-V: Prints the version information.
-v: Verbose the output as they are executed.
ifup
<<ifdown
 ifdown command to bring network interface down, not allowing the user to transmit and receive data. Technically it places the network interface in a state where it cannot transmit or receive data. It is used to configure network interfaces based on interface definitions in the file /etc/network/interfaces.
sudo ifdown -av , sudo is used for permissions, -a to work on all devices and v for verbose the output.
-a(–all): This option is used to bring all the interface down which are defined in /etc/network/interfaces.
–allow=CLASS: This option will only allow interfaces listed in an allow-CLASS line in /etc/network/interfaces to be acted upon.
-v(–verbose): Show or verbose commands as they are executed.
-V(–version): Prints the Copyright and version information.

ifdown
 
<<ping
PING (Packet Internet Groper) command is used to check the network connectivity between host and server/host. This command takes as input the IP address or the URL and sends a data packet to the specified address with the message “PING” and get a response from the server/host this time is recorded which is called latency. Fast ping low latency means faster connection. Ping uses ICMP(Internet Control Message Protocol) to send an ICMP echo message to the specified host if that host is available then it sends ICMP reply message. Ping is generally measured in millisecond every modern operating system has this ping pre-installed. 
sudo ping -v To get ping version installed on your system. 
ping www.geeksforgeeks.org To stop pinging we should use ctrl+c otherwise it will keep on sending packets. 
ping -c 5 www.geeksforgeeks.org Controlling the number of pings
ping -s 40 -c 5 www.geeksforgeeks.org Controlling the size of packets send
ping -i 2 www.geeksforgeeks.org Changing the time interval
ping -c 5 -q www.geeksforgeeks.org To get only summary
ping -w 3 www.geeksforgeeks.org To Timeout PING
ping -f www.geeksforgeeks.org To send packets as soon as possible. This is used to test network performance. 
ping -c 5 -W 3 www.geeksforgeeks.org Sets time to wait for a response. 
ping -c 5 -p ff www.geeksforgeeks.org We can fill data in packet using -p option. Like -p ff will fill packet with ones.
ping -c 5 -M want www.geeksforgeeks.org It is a simple protocol to find out the maximum MTU(Maximum Transmission Unit) a TCP path can take. 
ping -c 5 -t 64 www.geeksforgeeks.org It is maximum hop a packet can travel before getting discarded.A value 0 will restricts packet to same host.

ping

<<ethtool
ethtool is a networking utility on Linux. It is used to configure Ethernet devices on Linux. ethtool can also be used to find a lot of information about connected Ethernet devices on your Linux computer.Ethtool is a Linux command line tool used to view and modify various parameters of the network interface controllers (NICs) on a system. It allows you to gather information about the NIC's driver, firmware version, link status, speed, duplex, and more. Additionally, ethtool can be used to configure various settings of the NIC, such as the speed and duplex mode, auto-negotiation, and wake-on-LAN (WOL) settings. Ethtool can be useful for diagnosing and troubleshooting network connectivity issues, as well as for optimizing network performance by fine-tuning the NIC's settings.
sudo ethtool --version you can check whether ethtool is installed already
sudo ethtool enp6s0 a lot of information about the network interface card
sudo ethtool -i enp6s0  to check for the driver used by one of your NIC
sudo ethtool -S enp6s0 Display Network Usage Statistics with ethtool:

ethtool

<<portbonding
Port bonding, also known as link aggregation or network bonding, is a technique used in computer networking to combine multiple physical network connections into a single logical connection. This is done to increase the overall bandwidth and reliability of the network connection.
In port bonding, two or more network ports on a device are linked together to form a single high-bandwidth connection. This can be accomplished using different techniques, such as Round-Robin, where the traffic is distributed equally between the ports, or Link Aggregation Control Protocol (LACP), which provides automatic detection and configuration of the port bonding.
Port bonding is commonly used in high-performance computing environments, data centers, and other settings where high network bandwidth and reliability are critical. It allows for improved network redundancy, load balancing, and fault tolerance, which can result in improved network performance and uptime.

portbonding

<<wget

Option	Function
wget <URL>	Download single file
wget -O <fileName> <URL>	Store with a different file name
wget --limit-rate=<Numberk> <URL>	Specify download rate/speed
wget -c <URL>	Complete the remaining downloaded file
wget -b <URL>	Download in background
wget --tries=<Number> <URL>	Set retrying attempts
wget -i <fileName>	Download multiple files
wget --mirror -p --convert-links -P ./local dir <webURL>	Download full website
wget --reject=<rejectingFile> <URL>	Reject a type of file
wget -Q<Value>m -i <fileName>	Quit downloading on exceeding certain limit
wget -r -A.<fileType> <webURL>	Download certain file type
wget -o <logFile> <URL>	Redirect downloading file to the log file

wget
<<curl
The curl command is a powerful command-line tool for transferring data to and from web servers. It supports a wide range of protocols, including HTTP, HTTPS, FTP, SMTP, and more.
Some of the common use cases for the curl command include:
Downloading files from the web: curl can be used to download files from web servers via HTTP, HTTPS, FTP, and other protocols.
Uploading files to the web: curl can also be used to upload files to web servers, such as for backup or synchronization purposes.
Debugging web services: curl can be used to test and debug web services, by sending requests and receiving responses from web servers.
Automating web-related tasks: curl can be used to automate various web-related tasks, such as monitoring website availability, testing web application functionality, and more.
Overall, curl is a versatile and powerful tool for working with web servers, and it can be particularly useful for automation and scripting of web-related tasks.

curl
<<ftp
FTP stands for File Transfer Protocol. It is a standard protocol used for transferring files over the internet between computers. FTP is commonly used for uploading and downloading files to and from web servers.
FTP works by establishing a connection between a client (such as a web browser or FTP client software) and a server (such as a web server or FTP server). The client sends commands to the server to initiate file transfers, create and delete directories, and perform other operations.
FTP supports two modes of operation: active and passive. In active mode, the client initiates the data connection to the server, while in passive mode, the server initiates the data connection to the client. Passive mode is more commonly used today, as it is more firewall-friendly.
FTP also supports authentication mechanisms such as usernames and passwords, and can use encryption to secure the data being transferred.
While FTP is still widely used, it has been largely replaced by more secure and efficient protocols such as SFTP (Secure File Transfer Protocol) and FTPS (FTP over SSL/TLS).

ftp
<<scp
scp (secure copy) command in Linux system is used to copy file(s) between servers in a secure way. The SCP command or secure copy allows secure transferring of files in between the local host and the remote host or between two remote hosts. It uses the same authentication and security as it is used in the Secure Shell (SSH) protocol. SCP is known for its simplicity, security and pre-installed availability.
scp –P port: Specifies the port to connect on the remote host.
scp –p: Preserves modification times, access times, and modes from the original file.
scp –q: Disables the progress meter.
scp –r: Recursively copy entire directories.
scp –S program: Name of program to use for the encrypted connection. The program must understand ssh(1) options.
scp –v: Verbose mode. Causes scp and ssh to print debugging messages about their progress. This is helpful in debugging connection, authentication, and configuration problems.

scp

<<rsync
rsync or remote synchronization is a software utility for Unix-Like systems that efficiently sync files and directories between two hosts or machines. One of them being the source or the local-host from which the files will be synced, the other one being the remote-host, on which synchronization will take place. There are basically two ways in which rsync can copy/sync data:
Copying/syncing to/from another host over any remote shell like ssh, rsh.
Copying/Syncing through rsync daemon using TCP.
Rsync is famous for its delta-transfer algorithm, in which it copies only the differences between the source files present in the local-host and the existing files in the destination or the remote host.
rsync local-file user@remote-host:remote-file
rsync Documents/ it will show list same as ls -l
rsync -avh Documents/ jathin The above command will copy/sync all the files and directories present in directory Documents to directory jathin. If the destination directory is not present (here jathin), rsync automatically creates one and copies all the data in it.
rsync -avhze ssh /foo user@remote-host:/tmp/Rsync using ssh: There are two different ways for rsync to contact a remote system:
Using a remote-shell program as the transport(such as ssh(Secure Shell) or rsh(Remote Shell)).
Contacting an rsync daemon directly via TCP.
rsync -avhe ssh --chown=USER:GROUP /foo user@remote-host:/tmp/Rsync with particular file permissions: If we want to sync files to the local or remote host with the permissions of the files being changed. The following command must be used.
rsync

<<rpm
In Linux, RPM (Red Hat Package Manager) is a package management system used for installing, updating, and removing software packages on Red Hat-based distributions such as Red Hat Enterprise Linux, CentOS, Fedora, and others.
RPM is designed to automate the process of installing, upgrading, and removing software packages, and it is used to manage both the software itself as well as its dependencies. Each software package is stored in an RPM file, which contains the software and all the necessary information to install it on a Linux system.
With RPM, you can install a package by simply running the command "rpm -i package_name.rpm", where "package_name.rpm" is the name of the RPM file. You can also use RPM to query information about installed packages, update packages, and remove packages from the system.
RPM also allows you to verify the integrity of installed packages, and it provides tools for managing software repositories, which are collections of packages that can be installed on a Linux system.
-q: It is used for querying any package.
-p: It is used for listing the capabilities that this package gives.
-R: This option is used for listing the capabilities over which the package depends.
rpm

<<yum
In Linux, YUM (Yellowdog Updater Modified) is a package management tool used for installing, updating, and removing software packages on Red Hat-based distributions such as Red Hat Enterprise Linux, CentOS, Fedora, and others.
YUM is based on RPM and is designed to simplify the process of managing software packages by resolving dependencies automatically. YUM is capable of downloading packages from repositories and installing them on the system along with their dependencies.
With YUM, you can install a package by simply running the command "yum install package_name", where "package_name" is the name of the package you want to install. YUM will automatically download the necessary package and all its dependencies from a configured repository.
YUM also provides other useful commands, such as "yum update" to update all installed packages, "yum remove" to remove a package, "yum list" to list all available packages, and more.
YUM allows you to configure multiple software repositories, each containing a different set of packages. This enables you to access a wide range of software packages from different sources and install them easily on your system. YUM is a powerful and flexible tool for managing software packages on Linux systems.

yum

<<apt
In Linux, APT (Advanced Package Tool) is a package management system used for installing, updating, and removing software packages on Debian-based distributions such as Debian, Ubuntu, and others.
APT is designed to simplify the process of managing software packages by resolving dependencies automatically. APT is capable of downloading packages from repositories and installing them on the system along with their dependencies.
With APT, you can install a package by simply running the command "apt-get install package_name", where "package_name" is the name of the package you want to install. APT will automatically download the necessary package and all its dependencies from a configured repository.
APT also provides other useful commands, such as "apt-get update" to update the package list from the configured repositories, "apt-get upgrade" to upgrade all installed packages, "apt-get remove" to remove a package, "apt-cache search" to search for packages, and more.
APT allows you to configure multiple software repositories, each containing a different set of packages. This enables you to access a wide range of software packages from different sources and install them easily on your system.
APT is a powerful and flexible tool for managing software packages on Linux systems, and it is widely used in Debian-based distributions.

apt

<<dpkg
In Linux, dpkg (Debian Package) is a low-level package management tool used for installing, configuring, and removing Debian packages on Debian-based distributions such as Debian, Ubuntu, and others.
dpkg is capable of installing and removing individual packages or groups of packages, and it can also be used to list installed packages and display package information.
With dpkg, you can install a package by running the command "dpkg -i package_name.deb", where "package_name.deb" is the name of the Debian package file. dpkg will install the package and its dependencies, but it will not automatically resolve any missing dependencies. If any dependencies are missing, dpkg will report an error and the installation will fail.
dpkg also provides other useful commands, such as "dpkg --configure package_name" to configure a package after installation, "dpkg --remove package_name" to remove a package, "dpkg -l" to list all installed packages, and more.
dpkg is a low-level tool and does not provide automatic dependency resolution, so it is typically used in conjunction with higher-level tools like APT to manage packages and dependencies more effectively. APT uses dpkg as its backend for package installation and removal.
dpkg

<<ssh
SSH (Secure Shell) is a protocol used for secure remote access to a computer or server over an unsecured network. SSH is commonly used in Linux and other Unix-like operating systems to provide secure access to remote shells, remote command execution, and file transfer.
When a user initiates an SSH connection to a remote computer, the connection is encrypted and authenticated using cryptographic methods. This ensures that the connection is secure and that the user's credentials are protected from eavesdropping or interception.
SSH can be used for various purposes, such as accessing a remote shell to execute commands on a remote computer, transferring files securely between computers, and establishing secure tunnels for accessing remote services.
To initiate an SSH connection, the user needs an SSH client, which is usually pre-installed on most Linux and Unix-like operating systems. The user also needs to know the remote computer's IP address or hostname, as well as the username and password or other authentication credentials required to access the remote system.
Overall, SSH provides a secure and reliable way to remotely access and manage Linux and other Unix-like systems, making it an essential tool for system administrators and other IT professionals.
ssh
<<telnet
Telnet is a protocol used for remote access to a computer or server over a network. It allows a user to establish a connection to a remote computer and interact with it as if they were sitting in front of it
Telnet has been widely used in the past for remote access, but its use has declined in recent years due to security concerns. Telnet sends data, including login credentials, in clear text, making it vulnerable to eavesdropping and interception. As a result, it is generally not recommended to use Telnet for remote access.
Instead, SSH (Secure Shell) is the preferred protocol for secure remote access. SSH uses encryption to protect the data transmitted over the network, providing a much more secure and reliable way to remotely access and manage systems.
While Telnet may still be supported on some systems for legacy reasons, it is generally recommended to use SSH or other secure protocols for remote access to avoid security risks.
telnet

<<ntp
NTP (Network Time Protocol) is a protocol used for synchronizing the time of computer clocks over a network. NTP allows computers to obtain accurate time information from a trusted time source and adjust their clocks accordingly.
Accurate time synchronization is important for many applications, such as financial transactions, scientific research, and network security. Without accurate time synchronization, it can be difficult to correlate events across multiple systems or ensure the accuracy of time-sensitive operations.
NTP uses a hierarchical system of time servers to ensure that time information is accurate and reliable. The highest-level servers obtain time information from atomic clocks or other trusted sources, and lower-level servers obtain time information from higher-level servers in the hierarchy. Clients can then obtain time information from one or more servers in the hierarchy, adjusting their clocks as needed to maintain accurate time.
NTP is widely used in Linux and other Unix-like operating systems to synchronize the time of system clocks with a trusted time source over a network. The protocol is designed to be scalable and resilient, allowing it to function effectively even in large and complex network environments.
Overall, NTP is a crucial tool for ensuring accurate time synchronization in networked environments, and it plays an important role in many applications and industries.
ntp
<<chrony
Chrony is a system daemon for Linux and other Unix-like operating systems that provides accurate timekeeping and synchronization with network time servers. Chrony is an alternative to the older NTP (Network Time Protocol) daemon and provides several improvements over NTP, including faster synchronization, better support for virtual machines, and improved accuracy.
Chrony uses a combination of techniques to synchronize the system clock with a network time server, including NTP and its own algorithm that adjusts the clock frequency to maintain accurate time. Chrony can operate in both server and client modes, allowing it to act as a time server for other systems or synchronize the time of local systems with a remote time server.
Chrony provides several features that make it a reliable and accurate time synchronization tool, such as support for multiple time sources, automatic selection of the best time source, and continuous monitoring of time sources for accuracy and reliability.
Chrony is widely used in Linux and other Unix-like operating systems to ensure accurate timekeeping and synchronization, particularly in environments where accurate time is critical, such as financial systems, scientific research, and network security.
Overall, Chrony is a powerful and flexible time synchronization tool that provides improved accuracy and reliability over traditional NTP implementations, making it an essential tool for many applications and industries.
chrony
<<dhcp
A DHCP (Dynamic Host Configuration Protocol) server is a network server that automatically assigns IP addresses and other network configuration information to client devices on a network. DHCP servers are commonly used in enterprise networks to simplify the management of IP address allocation and ensure that each device on the network has a unique IP address.
When a device joins a network that has a DHCP server, the server automatically assigns an IP address to the device based on available addresses in a predefined pool. The DHCP server can also provide other network configuration information, such as the subnet mask, default gateway, and DNS server addresses. This makes it easier for network administrators to manage network configurations and ensures that all devices on the network have the correct configuration settings.
DHCP servers typically work in conjunction with DHCP clients, which are built into many modern operating systems and networking devices. The client sends a request to the DHCP server when it joins the network, and the server responds with the appropriate configuration information.
Overall, a DHCP server is a critical component of a modern network infrastructure, as it simplifies the management of IP address allocation and helps to ensure that each device on the network has a unique IP address and the correct network configuration information.
dhcp

<<firewall
A firewall is a network security system that monitors and controls incoming and outgoing network traffic based on a set of predefined security rules. The primary purpose of a firewall is to protect a network from unauthorized access and attacks by filtering traffic that flows through it.

Firewalls can be implemented as hardware devices, software programs, or a combination of both. They can be configured to allow or deny traffic based on a range of criteria, including source and destination IP addresses, port numbers, protocols, and other characteristics of the network traffic.

Firewalls can be used to achieve several security objectives, including:

Preventing unauthorized access: Firewalls can block traffic from unauthorized sources and prevent attackers from gaining access to the network.

Controlling access: Firewalls can control access to specific network resources, such as servers, applications, and data.

Detecting and blocking attacks: Firewalls can detect and block known attacks, such as malware, viruses, and other types of malicious traffic.

Monitoring traffic: Firewalls can monitor network traffic and generate alerts for suspicious or anomalous behavior.

Enforcing security policies: Firewalls can enforce security policies that specify how traffic is allowed to flow through the network and what types of traffic are permitted.

Overall, the use of a firewall is to protect a network from unauthorized access and attacks by filtering traffic that flows through it, making it an essential component of network security infrastructure.

firewall

<<ldap
LDAP (Lightweight Directory Access Protocol) is a protocol used for accessing and maintaining distributed directory information services over an IP network. LDAP is commonly used in enterprise environments to centralize the management of user accounts, passwords, and other directory-related information.

In Linux, OpenLDAP is a popular implementation of the LDAP protocol that provides a lightweight and efficient way to manage user accounts and other directory-related information. OpenLDAP can be used to authenticate users and provide single sign-on capabilities across multiple systems and applications.

Here are some of the key features and uses of LDAP in Linux:

Centralized directory management: LDAP provides a centralized repository for storing and managing directory information, including user accounts, passwords, and other related data.

Scalability: LDAP is designed to be scalable, allowing it to handle large numbers of users and directory-related information with ease.

Security: LDAP supports a range of security features, including encryption, authentication, and access control, which help to ensure the confidentiality, integrity, and availability of directory information.

Integration with other systems: LDAP can be integrated with other systems and applications, such as email servers, web servers, and file servers, to provide a single sign-on experience for users.

Standardization: LDAP is an open standard, which means that it is widely supported by a range of software vendors and can be used across multiple platforms and operating systems.

Overall, the use of LDAP in Linux provides a centralized and efficient way to manage directory-related information, making it an essential component of many enterprise environments.
ldap

<<fdisk
fdisk is a command-line utility used in Linux and other Unix-like operating systems for managing disk partitions. It is used to create, delete, resize, and modify partitions on a hard disk.
When you run the fdisk command, it displays the partition table of the specified disk. You can then use various commands to create, delete, or modify partitions. Some of the commonly used commands in fdisk are:
n: Used to create a new partition
d: Used to delete a partition
p: Used to print the partition table
w: Used to write the changes to the disk
Here's an example of how to use fdisk to create a new partition on a hard disk:
Open a terminal and type sudo fdisk /dev/sda to launch fdisk and specify the hard disk you want to partition (in this case, /dev/sda).
Type p to print the current partition table and make sure there is unallocated space on the disk.
Type n to create a new partition.
Follow the prompts to specify the partition type, start and end sectors, and other options.
Type w to write the changes to the disk and exit fdisk.
After creating the new partition, you can use the mkfs command to format the partition with a filesystem such as ext4 or NTFS, and then mount the partition to make it accessible.
fdisk
<<lvm
LVM stands for Logical Volume Manager, a tool used for managing disk storage in Linux and other Unix-like operating systems. LVM provides a layer of abstraction between the physical disks and the file systems, allowing you to create, resize, and move logical volumes (LVs) dynamically without affecting the underlying physical disks or file systems.
Here are the main components of LVM:
Physical Volume (PV): A physical disk or disk partition that is used by LVM. Each PV is divided into one or more physical extents (PEs).
Volume Group (VG): A collection of one or more PVs that are combined into a single storage pool. A VG can be resized by adding or removing PVs.
Logical Volume (LV): A virtual disk that is created from one or more PEs in a VG. An LV can be resized or moved to a different PV within the same VG.
LVM Metadata: A small portion of the disk space that is reserved for storing information about the LVM configuration, such as the location of the PVs and LVs.
LVM provides several benefits, including:
Flexibility: LVM allows you to manage storage space dynamically, without the need to repartition the physical disks or move data between them.
Resilience: LVM provides features such as mirroring (RAID-1) and striping (RAID-0) to protect against disk failures.
Performance: LVM can improve disk I/O performance by allowing you to stripe data across multiple disks.
To use LVM, you need to install the LVM packages and create PVs, VGs, and LVs using the pvcreate, vgcreate, and lvcreate commands, respectively. You can then format the LVs with a file system and mount them as you would with regular disks.
lvm
<<fsck
fsck stands for File System Consistency Check. It is a command-line utility in Linux and other Unix-like operating systems that checks and repairs the consistency of a filesystem. When a filesystem is not unmounted cleanly or is not shut down properly, there may be inconsistencies in the filesystem, such as lost files, corrupted data, or unused blocks.
fsck can be used to scan the filesystem for inconsistencies and repair them. The fsck command can be run on both the root filesystem and on other mounted filesystems.
The basic syntax of the fsck command is:
fsck [options] filesystem
Here, filesystem is the name of the filesystem to be checked. Some commonly used options for fsck are:
-a: Automatically repair the filesystem without prompting for confirmation
-y: Assume "yes" to all prompts and automatically repair any issues
-r: Interactively repair the filesystem, prompting the user for confirmation before making changes
-V: Verbose output, displaying more detailed information during the check and repair process.
Here's an example of how to use the fsck command to check and repair a filesystem:
Open a terminal and unmount the filesystem you want to check with the following command:
umount /dev/sda1
Here, /dev/sda1 is the device name of the filesystem to be checked.
Run the fsck command with the appropriate options to check and repair the filesystem:
fsck -y /dev/sda1
This will automatically repair any issues found in the filesystem without prompting for confirmation.
Once the fsck command has completed, remount the filesystem with the following command:
mount /dev/sda1 /mnt
Here, /mnt is the mount point for the filesystem.
fsck

<<nfs
NFS (Network File System) is a protocol for sharing files between Linux and Unix systems over a network. It allows you to mount remote file systems on your local machine and access them as if they were part of your local file system. With NFS, you can share files and directories between multiple servers and clients, making it a popular choice for file sharing in enterprise environments.

To use NFS, you need to have the NFS server and client packages installed on your Linux system. Once you have installed the required packages, you can configure NFS to share a directory on the server and mount it on the client machine.

Here are the basic steps to configure NFS on Linux:

Install the NFS server and client packages on the server and client machines respectively.

Create a directory that you want to share on the server machine and set the appropriate permissions for it. For example:

bash
Copy code
mkdir /shared
chmod 777 /shared
Configure the NFS server by editing the /etc/exports file and adding an entry for the directory to be shared. For example:

bash
Copy code
/shared 192.168.1.0/24(rw,sync,no_subtree_check)
Here, /shared is the directory to be shared, 192.168.1.0/24 is the IP address range of the clients that are allowed to access the shared directory, and rw (read-write), sync, and no_subtree_check are the mount options for the shared directory.

Restart the NFS server to apply the changes:

Copy code
systemctl restart nfs-server
On the client machine, mount the shared directory using the mount command. For example:

bash
Copy code
mount -t nfs server:/shared /mnt
Here, server is the IP address or hostname of the NFS server and /mnt is the local mount point for the shared directory.

Access the shared directory on the client machine and verify that you can read and write files to it.

NFS is a powerful and flexible protocol that can be used for a variety of file sharing use cases. However, it is important to ensure that proper security measures are in place to prevent unauthorized access to shared files and directories.
nfs

<<samba server
Samba is an open-source software suite that provides file and print services to SMB/CIFS (Server Message Block/Common Internet File System) clients. It allows you to share files and printers between Linux, Windows, and other operating systems over a network.
To set up a Samba server on Linux, you can follow these steps:
Install the Samba package using your distribution's package manager. For example, on Ubuntu, you can run:
Copy code
sudo apt-get install samba
Create a directory that you want to share on the Samba server and set the appropriate permissions for it. For example:
bash
Copy code
sudo mkdir /srv/samba/share
sudo chown nobody:nogroup /srv/samba/share
sudo chmod 777 /srv/samba/share
Here, /srv/samba/share is the directory to be shared, and the nobody user and nogroup group are used for security reasons.
Configure Samba by editing the /etc/samba/smb.conf file. Here's a basic configuration that allows read/write access to the shared directory:
java
Copy code
[share]
path = /srv/samba/share
writable = yes
guest ok = yes
read only = no
create mask = 0777
directory mask = 0777
Here, [share] is the name of the share, path is the path to the shared directory, writable allows write access, guest ok allows guest access without a password, read only is set to no, and create mask and directory mask are used to set permissions on newly created files and directories.
Restart the Samba service to apply the changes:
sudo systemctl restart smbd
On a client machine, connect to the Samba share using the file manager or the smbclient command. For example:
smbclient //server/share
Here, server is the hostname or IP address of the Samba server, and share is the name of the shared directory.
Access the shared directory and verify that you can read and write files to it.
Samba is a powerful and flexible protocol that can be used for a variety of file sharing use cases. However, it is important to ensure that proper security measures are in place to prevent unauthorized access to shared files and directories.
samba server

<<root
In Linux, "root" is the superuser account that has complete control over the system. The root user can perform any system task, including installing and removing software, modifying system files and settings, and managing user accounts and permissions.
By default, the root account is disabled in most Linux distributions, and users are encouraged to use regular user accounts for everyday tasks. This is because running as root can be dangerous if a user accidentally executes a command that could harm the system or compromise its security.
To use the root account in Linux, you typically need to first enable it and set a password. This can be done by running the following command:
Copy code
sudo passwd root
You will be prompted to enter a new password for the root account. Once the password is set, you can switch to the root account using the su (short for "switch user") command:
Copy code
su -
You will be prompted to enter the root password. After entering the password, you will be logged in as the root user and can perform any task with full system privileges.
It is generally recommended to use the root account sparingly and only when necessary, and to use regular user accounts with restricted permissions for everyday tasks. This can help to minimize the risk of accidental damage to the system and improve overall security.
root

<<absolute path
In computing, an absolute path and a relative path are two ways of specifying the location of a file or directory in a file system.
An absolute path specifies the exact location of a file or directory from the root directory of the file system. It begins with a forward slash (/) on Unix-based systems (such as Linux) or a drive letter followed by a backslash (\) on Windows systems. For example, the absolute path of a file called myfile.txt located in the /home/user/documents directory on a Linux system would be /home/user/documents/myfile.txt.
absolute path

<<relative path
A relative path, on the other hand, specifies the location of a file or directory relative to the current working directory. It does not begin with a forward slash or a drive letter. Instead, it uses one or two dots (. and ..) to indicate the current directory and the parent directory, respectively. For example, if the current working directory is /home/user, the relative path to a file called myfile.txt located in the documents directory would be documents/myfile.txt.
Relative paths are often shorter and easier to read than absolute paths, but they can be more prone to errors if the current working directory is not known or changes frequently. Absolute paths, on the other hand, are always unambiguous and provide a complete specification of the location of a file or directory in the file system.
relative path

<<inode
An Inode number is a uniquely existing number for all the files in Linux and all Unix type systems.
When a file is created on a system, a file name and Inode number is assigned to it.
Generally, to access a file, a user uses the file name but internally file name is first mapped with respective Inode number stored in a table.
Note: Inode doesn't contain the file name. Reason for this is to maintain hard-links for the files. When all the other information is separated from the file name then only we can have various file names pointing to the same Inode.
inode

<<soft link
Symbolic Links
What is a Symbolic Link:
A symbolic or soft link is a string which is the pathname of the original file, in other words, a shortcut as we know it in Windows Operating System. This kind of link references virtual or path location of the original file but not the physical location.

Properties
The filename in the soft link is in essence the pathname to the original file, therefore the size of this link is only the length name of the file link. It is not the size of the original file.

Advantages:
The main advantage, as we just saw, is that it will not take much space since its size will be smaller.
Another advantage is that it can be created for directories and for different file systems.

Disadvantages:
Some real big disadvantages are that if the file is renamed, moved or deleted the link-file will become useless becoming a hanging link.

Command:
$ ln -s [original filename] [link name]
softlink

<<hard link
What is a Hard Link?
The first way is with a Hard Link. This way of making a connection is through what is called the indode, which is in essence is the reference to the physical location of the file. This means that the link-file has the same indode value as the original-file, making it a sort of "moving copy of the original-file".

Properties:
The properties of the hard link are that the link-file has the same size as the original file since it is a reference to the same location of the original file therefore not only a reference to it but also the same properties.

Advantages:
Since it is a reference to the file's physical location and not just to its current location, the link will remain available and will not be affected even if the original file is moved, renamed or deleted. In other words, if any of this things occur, the link-file will still access the information in the original file. The link-file will still work.

Disadvantages:
Hard Link's main disadvantage is it can not be created for a directory nor it can be created for files on different file systems.

Command:
$ ln [original filename] [link name]
hard link

<<ack
What is ACL ?
Access control list (ACL) provides an additional, more flexible permission mechanism for file systems. It is designed to assist with UNIX file permissions. ACL allows you to give permissions for any user or group to any disc resource.
Use of ACL :
Think of a scenario in which a particular user is not a member of group created by you but still you want to give some read or write access, how can you do it without making user a member of group, here comes in picture Access Control Lists, ACL helps us to do this trick.
Basically, ACLs are used to make a flexible permission mechanism in Linux.
From Linux man pages, ACLs are used to define more fine-grained discretionary access rights for files and directories.
setfacl and getfacl are used for setting up ACL and showing ACL respectively.
acl

<<wildcards

Percent ( % ) in a wildcard
The percent symbol is used in SQL to match any character (including an underscore) zero or more times.
Asterisk ( * ) in a wildcard
The asterisk in a wildcard matches any character zero or more times. For example, "comp*" matches anything beginning with "comp," which means "comp," "complete," and "computer" are all matched.
Question mark ( ? ) in a wildcard
A question mark matches a single character once. For example, "c?mp" matches "camp" and "comp." The question mark can also be used more than once. For example, "c??p" would match both of the above examples. In MS-DOS and the Windows command line, the question mark can also match any trailing question mark zero or one time. For example, "co??" would match all of the above matches, but because they are trailing question marks would also match "cop" even though it's not four characters.
Open and close brackets ( [ ] ) in a wildcard
With Unix shells, Windows PowerShell, and programming languages that support regular expressions, the open and close bracket wildcards match a single character in a range. For example, [a-z] matches any character "a" through "z," which means anything not in that range, like a number, would not be matched.
wildcards

<<gzip
gzip file.txt it is used to decrease the size of the file 
gzip -d file.txt.gz it is used to unzip the file
gzip

<<java life cycle
The Java application life cycle can be illustrated as in Figure 4.1, “The Java Life Cycle”. We can use any text editor to create the high-level Java text file. This file is saved as a .java file on the disk. We then compile this text file using the Java compiler, which result in a .class file being created on the disk. The .class file contains the bytecodes. The file is then loaded into memory by the class loader. The bytecode verifier confirms that the bytecodes are valid and not hostile. Finally, the JVM reads the bytecodes in memory and translates them into machine code.
you can see image in downloads
Writing the Java Program: In this phase, a programmer writes a Java program using any text editor or Integrated Development Environment (IDE) such as Eclipse, NetBeans, or IntelliJ IDEA.
Compiling the Java Program: In this phase, the Java source code is compiled into Java bytecode by the Java Compiler. The Java Compiler verifies the syntax and correctness of the Java code and generates Java bytecode that can be executed by the Java Virtual Machine (JVM).
Loading: In this phase, the class loader loads the bytecode of the compiled Java program into the JVM's memory.
Linking: In this phase, the JVM verifies the bytecode to ensure that it is valid and that all dependencies are resolved. It also allocates memory for the class variables and initializes them with their default values.
Initialization: In this phase, the static variables and static blocks of the Java program are initialized.
Execution: In this phase, the main method of the Java program is invoked, and the Java program starts executing. The Java program executes its code line by line until it terminates or throws an exception.
Unloading: In this phase, the JVM removes the Java program's bytecode and frees the memory allocated by it.
These phases represent the typical Java program lifecycle, which is performed automatically by the JVM.

The overall aim of the Java lifecycle is to execute a Java program in a reliable, efficient, and secure manner. The Java lifecycle consists of several phases, including writing the Java program, compiling the Java code, loading the class files, linking, initializing, and executing the Java program.
During each phase of the Java lifecycle, various checks and verifications are performed to ensure that the Java program is executed correctly and safely. For example, the Java compiler checks the syntax and correctness of the Java code, the class loader loads the bytecode of the compiled Java program into the JVM's memory, and the JVM verifies the bytecode to ensure that it is valid and that all dependencies are resolved.
By following the Java lifecycle, Java programs can be executed in a consistent and predictable manner, regardless of the underlying platform or operating system. The Java lifecycle also provides a secure execution environment, with built-in mechanisms for memory management, exception handling, and security. Overall, the aim of the Java lifecycle is to ensure that Java programs are executed efficiently, reliably, and securely.

#installing of java in ubuntu

sudo apt install default-jre it is used for java installation in ubuntu system
whereis java to see where is java installed
sudo apt list --installed to see the list of installed java

javac:
javac is command or tool which is used to compile java file.It also known as java compiler(javac).
It phisically exists in the system at java installed folder.
To compile any java file we use it as: javac (java file name).java

java:
java  is also a command or tool just like javac but it is used to run a java file.
It phisically exists in the system at java installed folder.
To run any java file we use it as: java (java class name)

javap:
javap is also a command or tool(important).It is used to see the structure of any pre defined or
user defined class.(means to see how many methods, variables, constants,constructor etc in a class)
It phisically exists in the system at java installed folder.

java life cycle

<<jvm
JVM, i.e., Java Virtual Machine.
JVM is the engine that drives the Java code.
Mostly in other Programming Languages, compiler produce code for a particular system but Java compiler produce Bytecode for a Java Virtual Machine.
When we compile a Java program, then bytecode is generated. Bytecode is the source code that can be used to run on any platform.
Bytecode is an intermediary language between Java source and the host system.
It is the medium which compiles Java code to bytecode which gets interpreted on a different machine and hence it makes it Platform/Operating system independent.
Reading Bytecode.
Verifying bytecode.
Linking the code with the library.

JVM generates a .class(Bytecode) file, and that file can be run in any OS, but JVM should have in OS because JVM is platform dependent.
Java is called platform independent because of Java Virtual Machine. As different computers with the different operating system have their JVM, when we submit a .class file to any operating system, JVM interprets the bytecode into machine level language.
JVM is the main component of Java architecture, and it is the part of the JRE (Java Runtime Environment).
A program of JVM is written in C Programming Language, and JVM is Operating System dependent.
JVM is responsible for allocating the necessary memory needed by the Java program.
JVM is responsible for deallocating memory space.

jvm

<<java application
mobile application- build android os
web based application- gmail,facebook etc
gaming application-2d games
distributed application-google myntra tcs 
cloud based application-building of aws etc
scientific application- scientfic calculations , mathematical calculations
enterprise and business application- clubs charity banks
desktop gui application - awt(abstract window toolkit),swing developing by using api(application program interface)
java application

<<java properties
simple
object oriented
Platform Independent and Architecture Neutral
portability
high performance
multithreaded
secure
robust
distributed
Interpreted
java properties

<<java project structure
source code - project structure- dependencies/libraries -test


<<maven
Maven is chiefly used for Java-based projects, helping to download dependencies, which refers to the libraries or JAR files. The tool helps get the right JAR files for each project as there may be different versions of separate packages
https://www.tutorialspoint.com/maven/maven_overview.htm
when code is written in java we need to deploy it java is written ,if server dont have java it wont work
it is based on pom (project object model all the project related information) it is in xml(extensible markup language) file
with maven we add dependencies(whatsapp - internet we have whatsapp we cat use without internet)
all the code cant be deployed so the code is converted intko jar or war
jar --> only java
war -->  along with java we have xml,html
ear --> combination of jar and ear
it is mostly used for java projects
maven is written in java
build tools
.net : visual stdio
c,c++: make file
java : Ant,maven,gradle

uses of maven 
it makes project easy to built
easy to add new dependencies
maven architecture
the tool helps get the right jar files for each project as there may be different versions of separate packages
source code -work space here we have goals mvn test mvn compile this is goal, after completing of this we get zar file ,we keep this jar file in local repo,from local repo to we can keep it in central or remote repo
maven life cycle
generate a resource - source code 
compile code - mvn compile 
test - mvn test
package - mvn package converting into jar file
install -mvn install jar file is in local repo
deploy (to server)
maven installation
java -version
mvm --version if we dont have it wont shown the it will ask to install or else
$ wget https://mirrors.estointernet.in/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
$ tar -xvf apache-maven-3.6.3-bin.tar.gz
$ mv apache-maven-3.6.3 /opt/
use this commands
maven built phases
-build lifecycle consists of phases
validate check all the things
compile
test
package .zar,.war
install jar to local repo   
deploy to deploy to remote repo
artifact id is a project name - and group of projects

#target folder

In Maven, the target directory is the default location where the build output is generated. When you run the mvn package command, Maven compiles your Java source code, runs any necessary tests, and packages the compiled code and resources into a distributable format such as a JAR or WAR file. The resulting build artifacts are stored in the target directory.
The contents of the target directory vary depending on the type of project being built and the plugins being used. For a standard Java project, the target directory typically contains the compiled Java classes, resource files, and any other files generated during the build process. If the project is a web application, the target directory may also contain a packaged WAR file.

#maven logs

Maven is a build automation tool used primarily for Java projects. When Maven builds a project, it generates logs that provide information on what it's doing during the build process. These logs can be useful for troubleshooting issues and identifying errors.
The logs generated by Maven are hierarchical and provide detailed information about the build process. They typically include information such as the version of Maven being used, the project being built, the plugins being used, and the goals being executed.
The logs are displayed in the console or terminal window when Maven is run. By default, Maven logs are displayed at the INFO level, which provides a summary of the build process. However, the log level can be changed to display more or less information.
Maven logs can also be redirected to a file using the -l or --log-file option. This can be useful for reviewing the logs at a later time or sharing them with others.
Overall, Maven logs provide valuable information about the build process and can be an essential tool for debugging issues and improving the efficiency of the build process.


maven

<<tomcat
Apache Tomcat is a web server and servlet container that is used to serve Java web applications. It provides an environment to run Java code on a web server, making it a popular choice for deploying Java-based web applications. Tomcat is open source software developed by the Apache Software Foundation and is widely used for hosting Java web applications. It supports the Java Servlet, JavaServer Pages (JSP), Java Expression Language (EL), and Java WebSocket technologies.
A servlet container, also known as a web container, is a software application that manages and processes servlets, which are Java programming language classes that dynamically process requests and responses in web applications. Servlet containers are responsible for managing the lifecycle of servlets, mapping servlets to URLs, and handling HTTP requests and responses. Some examples of servlet containers include Apache Tomcat, Jetty, and JBoss.
A Servlet is a Java program that runs inside a web server, and is designed to receive and respond to client requests. Servlets typically implement the HTTP protocol and provide web services to clients through the exchange of request/response messages. They can dynamically generate web content, such as HTML pages, in response to user requests. Servlets are commonly used to build dynamic web applications and are an essential component of the Java web application stack.

sudo apt update
sudo apt install default-jdk
java -version
sudo useradd -r -m -U -d /opt/tomcat -s /bin/false tomcat
-r: Create a system account, which means it is a special user account that is not intended for interactive login sessions.
-m: Create a home directory for the user. In this case, the home directory is set to /opt/tomcat.
-U: Create a new group with the same name as the user. This is useful for managing file permissions and ownership for the user.
-d: Specify the home directory for the user, which in this case is set to /opt/tomcat.
-s: Specify the user's login shell. In this case, it is set to /bin/false, which means that the user cannot log in to the system.
This command is often used when setting up Tomcat as a service to run in the background.
wget -c https://downloads.apache.org/tomcat/tomcat-9/v9.0.34/bin/apache-tomcat-9.0.34.tar.gz
sudo tar xf apache-tomcat-9.0.34.tar.gz -C /opt/tomcat
sudo ln -s /opt/tomcat/apache-tomcat-9.0.34 /opt/tomcat/updated
sudo chown -R tomcat: /opt/tomcat/*
sudo sh -c 'chmod +x /opt/tomcat/updated/bin/*.sh'
sudo nano /etc/systemd/system/tomcat.service
[Unit]
Description=Apache Tomcat Web Application Container
After=network.target

[Service]
Type=forking

Environment="JAVA_HOME=/usr/lib/jvm/java-1.11.0-openjdk-amd64"
Environment="CATALINA_PID=/opt/tomcat/updated/temp/tomcat.pid"
Environment="CATALINA_HOME=/opt/tomcat/updated/"
Environment="CATALINA_BASE=/opt/tomcat/updated/"
Environment="CATALINA_OPTS=-Xms512M -Xmx1024M -server -XX:+UseParallelGC"
Environment="JAVA_OPTS=-Djava.awt.headless=true -Djava.security.egd=file:/dev/./urandom"

ExecStart=/opt/tomcat/updated/bin/startup.sh
ExecStop=/opt/tomcat/updated/bin/shutdown.sh

User=tomcat
Group=tomcat
UMask=0007
RestartSec=10
Restart=always

[Install]
java -version (first check the java version)
wget https://dlcdn.apache.org/tomcat/tomcat-9/v9.0.74/bin/apache-tomcat-9.0.74.tar.gz  (we need to search in the web and need to find the required tom cat and we need to copy the link)
tar -xzf apache-tomcat-9.0.74.tar.gz (This is a command that extracts the contents of a compressed file named apache-tomcat-9.0.74.tar.gz in the current directory. The command has three options:
•  -x means extract the files from the archive.
•  -z means decompress the archive using gzip.
•  -f means use the following filename as the archive name)
sudo mkdir /opt/tomcat (making tomcat directory in opt)
sudo mv apache-tomcat-9.0.74/* /opt/tomcat/ (moving download file to the tomcat directory)
cd /opt/tomcat/ (going to the file)
cd conf (opening conf)
sudo nano tomcat-users.xml (here we need to add some data in it to get login <role rolename="admin-gui"/>
<user username="jathin" password="Kawasakininja@123" roles="admin-gui"/>
in place of admin we can add manger also some time with manager we are getting issue)
cd bin (we need to go to the been by comming back) 
sh startup.sh (we need to run the startup.sh  file)
sudo nano /etc/tomcat9/tomcat-users.xml (some time we get issue we need to add data here and run in bin <role rolename="admin-gui"/>
<user username="jathin" password="Kawasakininja@123" roles="admin-gui"/>
)

Linux: The log will be present by default at /var/log/tomcat/*.log. In some cases, you will also find it at /opt/tomcat/logs.
Windows: In Windows, the logs will be present at the location where your software is, something like this: C:\program files\apache software foundation\apache-tomcat{ver}\logs\catalina.out
Docker: By default, the logs will be printed to STDOUT and STDERR so that the logging agents on the nodes can get these logs and push them to a centralized place.
Kubernetes: You can configure your logging agent to read the logs from the standard output and standard error from the containers.
#Tomcat logs 
are the files that keeps information related to web server issues and server tasks in proper files in given locations. It’s important to understand logging on the Tomcat server because this tells you if there’s any issue with the server and if so, whether it’s functional or performance related. There can be multiple types of logs segregated into different file locations. Knowing these locations will help you debug issues faster.
Tomcat global logs, called Catalina logs, are some of the most important log types present. There are others too, like application logs and localhost logs. I’ll be discussing all of these and how they can help you. But before that, let’s look at the configuration needed to enable logging in Tomcat.
By default, the Tomcat application server uses a nonsecure port 9080 and secure port 9443, along with ports 9005, 9009, and 8082. 
#Enable Tomcat Logging
To collect Tomcat logs, you first need to enable logging. Logging in Tomcat is handled by the Java Utility Logging Implementation, also known as JULI. JULI is enabled by default, and you can perform this configuration using the logging configuration file option -Djava.util.logging.config.file=”logging.properties”:
Globally: ${catalina.base}/conf/logging.properties. This file is specified by the java.util.logging.config.file system property. If you don’t configure it, the default file will be ${java.home}/lib/logging.properties in the JRE.
In the web application: Use WEB-INF/classes/logging.properties.
If you don’t want to use JULI, there are other logging frameworks like log4j, log4j2, or Logback. Let’s take log4j as example.
To configure log4j for logging, just create a log4j.properties file and use log4j.jar. You can put the log4j.properties file into the $CATALINA_BASE/lib folder.
There are multiple options provided to configure the log4j logs. A few of the important ones are log4j.appender.CATALINA.File, which configures the location of the file; log4j.appender.CATALINA.Encoding can configure the encoding; and the UTF-8.log4j.appender.LOCALHOST.DatePattern option, which helps in configuring the log rotation based on date pattern.
For configuring Manager, Localhost, Host-Manager, and Catalina logs, the configuration parameters start with prefixes: log4j.appender.MANAGER for Manager, log4j.appender.HOST-MANAGER for Host Manager, log4j.appender.LOCALHOST for localhost, and log4j.appender.CATALINA for Catalina. You can see the exact instructions in the Tomcat documentation.
#Tomcat Log Levels
As with any other logging framework, JULI also has log levels that define the severity of the log and the information it contains. JULI log levels consist of:
SEVERE: Information pertaining to a serious problem
WARNING: Could be an issue in the future
INFO: Informational messages
CONFIG: Configuration information (listed when log level is set to config)
FINE: Trace logs
FINER: More-detailed trace logs
FINEST: Highest-detailed trace logs
These log levels are sorted in the order of the details that they feature. So, SEVERE has only very important information, while FINEST presents you with the most information. Please note that having a lot of information in your logging can have a performance impact on your disk’s I/O and increase your system’s load average.
Log levels can be set for different handlers from the logging.properties file found in $CATALINA_BASE/conf:
Tomcat Log File Types
There are multiple log files that Tomcat produces for its different components. The most important are all the Catalina logs since you can easily find most information in these files.

Catalina Logs
Tomcat Catalina produces two types of Catalina log files: catalina.log and catalina.out.

Catalina.log
This is the global log file used to save information like server admin operations, for example, server stop, start, restart, deployment of a new application, and failure of a subsystem. If you want to make changes to this log format, do so in the java.util.logging.SimpleFormatter.format variable. You can configure the location of the file in the $CATALINA_OUT parameter, or it will be present by default at CATALINA_HOME/logs.

Catalina.out
The servlet and error logs will be written in the catalina.out log file. This file can also contain thread dumps and uncaught exceptions.

You can make changes to the format of catalina.out in the java.util.logging.SimpleFormatter.format variable. The location of this file is defined in the $CATALINA_OUT parameter or is present by default at CATALINA_HOME/logs.

Localhost Log
These are the logs that track the activity log of your web application, e.g., the transactions between the application server and clients. These log files are present in the Tomcat home directory inside the logs folder.

Localhost Access Log
There can be cases where the application server has to make a call to another service, i.e., outbound HTTP requests. The localhost access log keeps track of all these requests. Localhost access logs are also found in the same location as the localhost logs inside the log directory of the Tomcat home directory.

Access Log
Tomcat access logs can give you information about who has access to your application, what resources were accessed, the IP, queries, date, etc. These are some of the most important logs for traffic analysis and can be configured via the server.xml file inside the conf directory in the Tomcat home directory.

Manager Log
These logs keep information on all the activities performed by the Tomcat manager, including lifecycle management, application deployment, etc. These logs can be configured in the logging.properties configuration file present at TOMCAT_HOME/conf.

The first step is to identify the location of the Tomcat installation directory. If you have installed Tomcat from default repositories, it should be installed under the /etc/tomcat{VERSION} directory. For the manual installation, you need to find the correct location.
Then edit server.xml file located under the Tomcat installation directory.
sudo nano /etc/tomcat9/server.xml 
Find the below content in the configuration file
<Connector port="8080" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
Replace port 8080 with your required port. For example, we are changing the default tomcat port to 8081.
    <Connector port="8081" protocol="HTTP/1.1"
               connectionTimeout="20000"
               redirectPort="8443" />
After making the above changes restart the tomcat service. You will see that now tomcat is started on port 8081 or the port you configured. Access tomcat on the new port in a web browser.

tomcat

<<nginx
Nginx (pronounced "engine x") is a high-performance web server, reverse proxy, and load balancer. It is open-source software that can be used as a standalone web server or can be used to proxy HTTP requests to other web servers. Nginx is known for its ability to handle a large number of concurrent connections and its low memory footprint compared to other web servers. It is commonly used in high-traffic websites, content delivery networks (CDNs), and web applications.
Nginx is used as a web server, reverse proxy, load balancer, and HTTP cache. It is commonly used to serve static and dynamic content, handle SSL/TLS encryption, and distribute traffic to multiple servers or applications. Nginx is often used in high-traffic websites, web applications, and APIs. It is also used as a proxy server for email protocols like IMAP, POP3, and SMTP, and for streaming media protocols like RTMP and HLS. Additionally, Nginx can be used to accelerate content delivery through its caching capabilities.
installation
sudo apt-get update  
sudo apt-get install nginx  
ps ?ef | grep nginx  
ps -ef | grep nginx  
sudo ufw app list
sudo ufw allow 'Nginx Full'  
sudo ufw allow 'Nginx HTTP'  
sudo ufw allow 'Nginx HTTPS'  
systemctl status nginx
NGINX is a web server, reverse proxy, and load balancer that is used to serve web content and handle high traffic volumes. It is widely used in production environments to enhance the performance, reliability, and security of web applications. With NGINX, you can efficiently distribute incoming traffic across multiple servers, cache static content, terminate SSL/TLS connections, and perform a range of other functions that are critical for web application delivery. NGINX is known for its high-performance, low-resource utilization, and easy configurability, making it a popular choice for many web development teams.

#nginx AS a reverse proxy
As a reverse proxy, Nginx can be used for a variety of purposes such as:
Load balancing: Nginx can distribute incoming traffic across multiple backend servers, helping to balance the load and increase performance and reliability.
SSL termination: Nginx can terminate SSL/TLS connections, offloading the processing of encryption/decryption from backend servers.
Caching: Nginx can cache content in memory or on disk, reducing the load on backend servers and improving response times for clients.
Compression: Nginx can compress content before sending it to clients, reducing the amount of data that needs to be transferred over the network.
Security: Nginx can act as a firewall, protecting backend servers from malicious traffic by blocking requests that do not meet certain criteria.
URL rewriting: Nginx can rewrite URLs, allowing requests to be directed to different backend servers based on the requested path or other criteria.

nginx

Apache vs. NGINX
@Apache	                                       #NGINX
@Apache runs on all Unix like systems such as Linux, BSD, etc. as well as completely supports Windows.
#Nginx runs on modern Unix like systems; however it has limited support for Windows.
@Apache uses a multi-threaded approach to process client requests.
#Nginx follows an event-driven approach to serve client requests.
@Apache cannot handle multiple requests concurrently with heavy web traffic.	
#Nginx can handle multiple client requests concurrently and efficiently with limited hardware resources.
@Apache processes dynamic content within the web server itself.	
#Nginx can't process dynamic content natively.
@Apache is designed to be a web server.
#Nginx is both a web server and a proxy server.
@Modules are dynamically loaded or unloaded, making it more flexible.	
#Since modules cannot be loaded dynamically, they must be compiled within the core software itself.
@A single thread can only process one connection.	
#A single thread can handle multiple connections.
@The performance of Apache for static content is lower than Nginx.
#Nginx can simultaneously run thousands of connections of static content two times faster than Apache and uses little less memory.

#application server and web server
weserver 
webserver is a computer system that hosts websites (website-webpages)
runs web server software (apache,http,microsftiis) should connect through internet
webserver in two parts sharedhosts and dedicated hosts
webserver-webclient-sends http request -it request some page- some time the page will be find some time it wont be present in it so it contact with some static data base and give the response (-send a servlet request-servlet means java program )-and contact application server and send a sevlet request -and application server sends  http responce to the web client.
webserver
apache http server,iis,lightps,sunjava system web server,jigsaw server
application server 
a server specifically designed to run applications
includes both hardware and software that provides an environment programs to run
used for : 
running web applications,hosting a hypervisor that manages virtual machines, distributing and monitorinng software updates,processing data sent from another server.
why we use application server 
providing processing power and  memory to run demanding applications
also provides the environment to run specific applications

<<catalina
In the context of Apache Tomcat, Catalina refers to the core Servlet/JSP container of Tomcat. It is responsible for handling incoming requests, managing the Servlet/JSP lifecycle, and providing HTTP service. In other words, Catalina is the component that implements the Servlet and JSP specifications, and it is the part of Tomcat that makes it a Servlet container.
catalina
<<journalctl
journalctl is a utility used to query and read the logs generated by systemd, a system and service manager used by many Linux distributions. The journalctl command is used to retrieve log messages from the system journal, which contains a record of events and messages generated by the kernel, system services, and other applications.
journalctl can be used to view, filter, and search the logs, and can display them in a variety of formats. It can also be used to view logs from remote systems that use systemd-journald for logging.
When systemctl or other systemd services fail to start or encounter issues during operation, journalctl can be used to view the logs and diagnose the problem.
journalctl
<<postgresql
The output you provided appears to be a result of running a query in a PostgreSQL database. It shows information about the existing databases and their access privileges. Here's a breakdown of the columns:

Name: The name of the database.
Owner: The user who owns the database.
Encoding: The character encoding used by the database (UTF8 in this case, which is a popular Unicode encoding).
Collate: The collation used for string comparison and sorting in the database (en_IN in this case, indicating English language with India-specific collation rules).
Ctype: The character classification used for character operations in the database (en_IN in this case, indicating English language with India-specific character classification rules).
ICU Locale: ICU (International Components for Unicode) locale, if any, used by the database.
Locale Provider: The library used for locale support (in this case, libc).
Access Privileges: The access privileges granted to specific roles for the database. The format is "role=privileges" (e.g., "=c/postgres" means the role "postgres" has the privilege to create in the database).
If you have any specific questions or need further assistance, please let me know!
In PostgreSQL, access privileges determine what actions can be performed on database objects (tables, views, functions, etc.) by different roles (users and groups). The access privileges are defined using a combination of permissions and roles.
Here's a breakdown of the access privileges in PostgreSQL:
Privileges: Privileges define the specific actions that can be performed on a database object. Some common privileges include:
SELECT: Allows reading data from a table or view.
INSERT: Allows inserting new data into a table.
UPDATE: Allows modifying existing data in a table.
DELETE: Allows deleting data from a table.
CREATE: Allows creating new objects (tables, views, etc.) within a schema.
DROP: Allows dropping (deleting) objects from a schema.
GRANT: Allows granting/restricting privileges to other roles.
USAGE: Allows using a schema or a database.
These are just a few examples, and there are additional privileges available in PostgreSQL.
Roles: Roles are used to group and manage database users. Roles can have privileges assigned to them, and users can be members of roles. Some commonly used roles in PostgreSQL include:
Superuser: A role with all privileges and no restrictions.
Database Owner: The role that owns a particular database and has all privileges on it.
Schema Owner: The role that owns a specific schema within a database and has all privileges on objects within that schema.
Public: A special role that represents all users and is granted certain default privileges.
Granting Privileges: Privileges can be granted to roles using the GRANT command. For example, to grant SELECT privilege on a table to a role, you can use:
sql
Copy code
GRANT SELECT ON table_name TO role_name;
Privileges can be granted at various levels, such as the database level, schema level, or object level.
Revoking Privileges: Privileges can be revoked from roles using the REVOKE command. For example, to revoke the SELECT privilege on a table from a role, you can use:
sql
Copy code
REVOKE SELECT ON table_name FROM role_name;
Similar to granting privileges, revoking privileges can be done at different levels.
It's important to note that access privileges in PostgreSQL can be quite granular, allowing for fine-grained control over database object permissions. By properly assigning privileges to roles, you can ensure that users have the necessary access and security within your database.
I hope this explanation clarifies the concept of access privileges in PostgreSQL. If you have any more specific questions, feel free to ask!
(https://aws.amazon.com/blogs/database/managing-postgresql-users-and-roles/) full data
to open db 
$ sudo -u postgres psql
to create db
create database mydb
to switch into mydb 
\c mydb
to create user
create user jathin_user with password 'jathin123';
create role 
create role readonly;
ALTER ROLE jathin_user SET ROLE readonly;
grant role
grant role readonly to jathin_user;
to remove role 
revoke readonly from jathin_user
if we need to remove the users first we need to revoke the granted permissions next we need to revoke the access privilages of the user from db
REVOKE ALL PRIVILEGES ON DATABASE mydb FROM jathin_user;
after this we can remove the user
DROP ROLE jathin_user

--> how to connect server to the pg admin
first we need to find where postgres.conf file present then we need to open the conf file and we need to find the #listen_addresses = 'localhost'	in the conf file then go to the pg admin give listen_addresses name to the server and then connect to it. in this way can connect server to the pgadmin 
postgresql

>>jenkins
This is the Debian package repository of Jenkins to automate installation and upgrade. To use this repository, first add the key to your system:    
curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | sudo tee \
/usr/share/keyrings/jenkins-keyring.asc > /dev/null
Then add a Jenkins apt repository entry:
echo deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] \
https://pkg.jenkins.io/debian-stable binary/ | sudo tee \
/etc/apt/sources.list.d/jenkins.list > /dev/null
Update your local package index, then finally install Jenkins:
sudo apt-get update
sudo apt-get install fontconfig openjdk-11-jre
sudo apt-get install jenkins
sudo systemctl status jenkins
sudo systemctl enable --now jenkins
sudo ufw allow 8080
sudo ufw enable
http://localhost:8080 search it in web
sudo cat /var/lib/jenkins/secrets/initialAdminPassword   ---- to get the login admin passwoed
then press install suggested plugins
jenkins

>>docker/dockered
difference between docker and dockered

Docker:
"Docker" is an open-source platform that allows developers to build, ship, and run applications inside containers. It provides a set of tools and a platform to create and manage containerized applications. Docker enables you to package an application and its dependencies into a single container image, making it portable and consistent across various environments.
The Docker platform consists of several components, including the Docker CLI (Command Line Interface), Docker Engine (daemon), Docker images, and Docker containers. The Docker CLI is used to interact with the Docker daemon and manage containers and images.
dockerd:
"dockerd" (Docker daemon) is the core component of the Docker platform. It is a system service that runs in the background and manages Docker containers and images. The Docker daemon is responsible for building, running, and monitoring containers. It listens to requests from the Docker CLI or other Docker client tools, such as Docker Compose, and performs the requested container operations.
In simpler terms, "dockerd" is the Docker engine that handles the actual container runtime and manages the container lifecycle. It is responsible for creating, starting, stopping, and deleting containers based on the commands issued by the Docker CLI or other Docker client tools.
In summary, "Docker" refers to the entire containerization platform, which includes the Docker CLI and the Docker Engine (dockerd). The Docker CLI is the interface used to interact with the Docker daemon (dockerd), which handles the actual container management and runtime operations.

docker/dockered

>>dockerdefault directory
By default, Docker stores most of its data inside the /var/lib/docker directory on Linux systems. 
dockerdafault
docker info | grep "Docker Root Dir"(to see the docker default directory path)

>>jenkinsdefault directory

In general, Jenkins stores its data, configurations, and job information in a directory known as the "Jenkins Home" directory. The location of the Jenkins Home directory depends on the installation method and operating system. Here are some common default locations:
Linux (Package Installation):
When Jenkins is installed using package managers like apt (Ubuntu/Debian) or yum (CentOS/RHEL), the Jenkins Home directory is usually located at /var/lib/jenkins/.
Windows (Installer):
On Windows systems, the Jenkins Home directory is typically located at C:\Program Files (x86)\Jenkins.
macOS (Installer):
On macOS systems, the Jenkins Home directory is usually located at /Users/Shared/Jenkins/Home.
Please note that these default locations may vary depending on the installation method and configuration. If you have installed Jenkins using a custom installer or using Docker, the Jenkins Home directory might be in a different location. Also, if you are using Jenkins in a cloud environment or a containerized setup, the directory structure could be different.
To find the exact location of your Jenkins Home directory, you can check the Jenkins configuration file or Jenkins system settings. For example, in a Linux environment, you can find the Jenkins Home directory path in the /etc/default/jenkins file (Ubuntu/Debian) or /etc/sysconfig/jenkins file (CentOS/RHEL).
Additionally, you can log in to your Jenkins dashboard as an administrator and navigate to "Manage Jenkins" > "Configure System." Under the "Home directory" section, you can see the path of the Jenkins Home directory.
Keep in mind that the information provided here is based on my last update in September 2021, and the default directory for Jenkins might change in newer versions or based on updates to installation methods. For the most up-to-date information, please refer to the official Jenkins documentation or the documentation specific to your Jenkins installation.

jnkinsdefault

>>dockered --help
This command will display a detailed list of options and their explanations that you can use with dockerd. The available options may include configuration parameters related to network settings, storage drivers, authentication, logging, security, and more.
Please note that the output may be lengthy, so you may want to scroll through it to find the specific options you are interested in. If you need to look up specific options or want to see the most up-to-date information, you can also refer to the Docker documentation or use the man dockerd command (on Linux) to see the manual page for dockerd.
Keep in mind that Docker and its components may have evolved since my last update, so I recommend checking the Docker documentation or running dockerd --help on your system to get the most current and relevant information.
dockered

>>docker --help
The output will display a list of global options that you can use with the docker command, such as --config, --context, --debug, --host, --log-level, --version, etc.

Below the options, you will find a list of available Docker commands. Each command represents a specific operation or action that you can perform with Docker, such as building images, creating containers, managing networks, and more. For example, some of the commands listed are attach, build, commit, cp, create, and so on.

To get more information about a specific Docker command, you can run docker COMMAND --help. Replace COMMAND with the actual command you want to learn more about. For example, docker build --help will display detailed information about the docker build command and its available options.

Keep in mind that the available commands and options may vary based on the version of Docker installed on your system. If you have a newer version of Docker, the list of commands and options may be different from what is shown above. For the most up-to-date and comprehensive information, I recommend checking the Docker documentation or running docker --help on your local system.
docker

>>garbagecollection
Docker uses layered container images, and each image layer is read-only. When a new container is created, a new writable layer is added on top of the read-only image layers. This writable layer allows changes to be made within the container without modifying the original image. This copy-on-write mechanism ensures efficient storage usage.
Regarding garbage collection, Docker relies on the storage driver to manage the storage of container images and layers. The storage driver is responsible for removing unused image layers and freeing up space when images or containers are deleted.
For example, if you delete a container, Docker will automatically remove the associated writable layer of that container. If an image is no longer used by any container or other images, Docker may remove that image's layers as well, depending on the storage driver configuration.
The specific behavior and garbage collection strategy may vary based on the storage driver being used. The default storage driver for Docker on Linux systems is usually "overlay2" or "aufs," but other options like "devicemapper," "btrfs," or "zfs" might be used in different setups.
It's essential to note that while Docker manages storage space through its storage driver, it's primarily concerned with image and container layers and not memory management within running containers. Memory management, including garbage collection for memory used by applications inside containers, is handled by the operating system and the container runtime, which is usually Docker Engine.
For more detailed information about Docker's storage management and garbage collection, including the specifics of different storage drivers and Docker configurations, it is best to refer to the official Docker documentation or resources specific to the version and configuration of Docker you are using.

garbagecollection

>>systemd

Systemd is a system and service manager for Linux-based operating systems. It is designed to provide a more efficient and reliable initialization and management system for the Linux kernel and user-space processes. Systemd replaces the traditional System V (SysV) init system, which was prevalent in older Linux distributions.
Key features and components of systemd include:
Initialization and Boot Process: Systemd is responsible for the system's initialization and boot process. It starts essential system services and manages the order in which they are launched during boot-up, making the boot process faster and more parallelized.
Service Management: Systemd can start, stop, restart, enable, and disable system services. Services can be managed as "units" in systemd, and their configuration files usually have a .service extension. This approach simplifies service management and makes it easier to define dependencies and relationships between services.
Dependency Management: Systemd handles dependencies between services, ensuring that necessary services are started or stopped in the correct order. This allows for more efficient parallelization during system boot and shutdown.
Logging and Journaling: Systemd includes the systemd journal, a centralized logging system that collects and stores log data from various services and applications. The journalctl command is used to access and view these logs.
System State Management: Systemd provides commands to manage the system's state, such as power management, system sleep and hibernate, and system shutdown.
User Session Management: Systemd can also manage user sessions, ensuring consistent and controlled user environment setup and teardown during user logins and logouts.
System Timer Management: Systemd offers a timer mechanism to schedule and run tasks at specific times or intervals, similar to cron jobs.
Systemd has been widely adopted in modern Linux distributions and is known for its fast boot times, efficient parallelization, and robust service management capabilities. It has received both praise and criticism over the years, but it has become a standard part of many Linux distributions due to its widespread usage and active development.
Systemd configuration files are located in directories such as /etc/systemd/system/ for system-level units and /usr/lib/systemd/user/ for user-level units. The main systemd configuration file is /etc/systemd/systemd.conf.
Overall, systemd plays a critical role in modern Linux distributions, and its introduction has significantly improved the boot performance and management capabilities of Linux systems.
systemd

>>journal(vs)systemctl

journalctl and systemctl are two different commands used in Linux-based systems to manage and view system-related information. They serve distinct purposes and are part of the systemd ecosystem.
journalctl:
journalctl is a command-line utility used to access and view logs from the systemd journal. The systemd journal is a centralized logging system that collects and stores log data generated by various services and applications running on the system. It provides a comprehensive and structured log of system events, including kernel messages, service start/stop events, and application logs.
Key features of journalctl:
Displays logs in a human-readable format with timestamps.
Supports various filtering options based on time, priority, unit (service), etc.
Allows for real-time log monitoring with the -f (follow) option.
Can display logs from the current boot (-b) or from a specific unit (-u).
Example usage:
# View all logs
journalctl
# Display logs for a specific service (unit)
journalctl -u nginx
# Monitor logs in real-time
journalctl -f
systemctl:
systemctl is a command-line utility used to manage the systemd system and service manager. It is used to control and interact with system services, view their status, start or stop them, enable or disable them on boot, and more. Systemd is a replacement for traditional System V (SysV) init system and provides advanced features for service management.
Key features of systemctl:
Controls the state of system services (start, stop, restart, enable, disable, etc.).
Views the status of services, including whether they are running or not.
Manages service units and can enable/disable services at boot time.
Logs service-related output and errors.
Example usage:
# Start a service
sudo systemctl start nginx
# Stop a service
sudo systemctl stop nginx
# Restart a service
sudo systemctl restart nginx
# Enable a service to start on boot
sudo systemctl enable nginx
# Disable a service from starting on boot
sudo systemctl disable nginx
# Check the status of a service
systemctl status nginx
In summary, journalctl is primarily used to view and manage logs generated by the systemd journal, while systemctl is used to control and interact with system services managed by the systemd system and service manager. Both commands are essential for monitoring and managing the state of the system and its services in a Linux-based environment.

journal(vs)systemctl

>>7filetypes How to Find Out File Types in Linux
In Linux, everything is considered as a file. In UNIX, seven standard file types are regular, directory, symbolic link, FIFO special, block special, character special, and socket. In Linux/UNIX, we have to deal with different file types to manage them efficiently.
In Linux/UNIX, Files are mainly categorized into 3 parts:
Regular Files
Directory Files
Special Files

We can test a file type by typing the following command:
file file.txt

We can pass a list of files in one file and we can specify using the -f option as shown below:
cat file.txt
file -f file.txt

Using the -s option we can read the block or character special file. 
file -s /dev/sda

Using -b option will not prepend filenames to output lines
file -b GFG.txt

Using -F option will use string as separator instead of “:”.
file -F '#' GFG.txt

Using -L option will follow symlinks (default if POSIXLY_CORRECT is set):
file -L stdin
open in /dev

We can use the –extension option to print a slash-separated list of valid extensions for the file type found.
file --extension GFG.rar

For more information and usage options, you can use the following command:
man file

Regular Files
Regular files are ordinary files on a system that contains programs, texts, or data. It is used to store information such as text, or images. These files are located in a directory/folder. Regular files contain all readable files such as text files, Docx files, programming files, etc, Binary files, image files such as JPG, PNG, SVG, etc, compressed files such as ZIP, RAR, etc. 

Directory Files
The sole job of directory files is to store the other regular files, directory files, and special files and their related information. This type of file will be denoted in blue color with links greater than or equal to 2. A directory file contains an entry for every file and sub-directory that it houses. If we have 10 files in a directory, we will have 10 entries in the directory file. We can navigate between directories using the cd command
We can find out directory file by using the following command:
ls -l | grep ^d 

Special Files
1. Block Files:
Block files act as a direct interface to block devices hence they are also called block devices.  A block device is any device that performs data Input and Output operations in units of blocks. These files are hardware files and most of them are present in /dev.
We can find out block file by using the following command:
ls -l | grep ^b

Character device files:
A character file is a hardware file that reads/writes data in character by character in a file. These files provide a serial stream of input or output and provide direct access to hardware devices. The terminal, serial ports, etc are examples of this type of file.
We can find out character device files by:
ls -l | grep ^c

Pipe Files:
The other name of pipe is a “named” pipe, which is sometimes called a FIFO. FIFO stands for “First In, First Out” and refers to the property that the order of bytes going in is the same coming out. The “name” of a named pipe is actually a file name within the file system. This file sends data from one process to another so that the receiving process reads the data first-in-first-out manner.
We can find out pipe file by using the following command:
ls -l | grep ^p

Symbol link files:
A symbol link file is a type of file in Linux which points to another file or a folder on your device. Symbol link files are also called Symlink and are similar to shortcuts in Windows. 
We can find out Symbol link file by using the following command:
ls -l | grep ^l

Socket Files:
A socket is a special file that is used to pass information between applications and enables the communication between two processes. We can create a socket file using the socket() system call. A socket file is located in /dev of the root folder or you can use the find / -type s command to find socket files.
find / -type s 
We can find out Symbol link file by using the following command:
ls -l | grep ^s

7filetypes

>>stdin stdout stderr
stdin, stdout, and stderr are standard streams in a computer's operating system and are used for handling input and output in a program. They play an essential role in the communication between a program and its environment (user or other software). These streams are available in most programming languages and are typically associated with the console or terminal where a program is executed.

stdin (Standard Input):
stdin is the standard input stream, which is used to receive input data into a program. This input can come from the user, a file, or another program. When a program reads from stdin, it processes the data provided to it. For example, if you have a program that reads user input, the data entered by the user will be received through stdin.

stdout (Standard Output):
stdout is the standard output stream, which is used to send regular output or information from a program. When a program writes to stdout, the data is usually displayed on the console or terminal where the program is running. This allows the user to see the program's output or capture it in a file if they redirect the output. Most print statements in programming languages write to stdout.

stderr (Standard Error):
stderr is the standard error stream, which is used to send error messages or diagnostics from a program. When a program encounters an error or needs to provide some diagnostic information, it writes to stderr. Like stdout, the data written to stderr is also typically displayed on the console or terminal. However, stderr is different from stdout in that it is not affected by output redirection, so error messages are still shown on the screen even if the regular output is being redirected to a file.
In summary, these standard streams provide a standardized way for programs to interact with their environment, allowing them to receive input, display regular output, and report errors or diagnostics. This design allows for consistency and ease of use in command-line programs, scripting, and other scenarios where input and output are essential.
stdin − It stands for standard input, and is used for taking text as an input.

stdout − It stands for standard output, and is used to text output of any command you type in the terminal, and then that output is stored in the stdout stream.

stderr − It stands for standard error. It is invoked whenever a command faces an error, then that error message gets stored in this data stream.
It should be noted that in Linux all these streams are treated as if they were files. Also, linux assigns unique values to each of these data streams.

0 = stdin

1 = stdout

2 = stderr

stdin

